{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f012cb60-ece3-4d23-a50d-35b0242d0df7",
   "metadata": {},
   "source": [
    "# Data Transformation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a299f497-518a-4f90-8ac0-6d0558dfdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bd207f-3cf0-4bb5-a55d-720bb017be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\638658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\638658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\638658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Download tokenizer\n",
    "nltk.download('stopwords')  # Download stopwords\n",
    "nltk.download('wordnet')  # Download WordNet (for lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c32c710-c3df-4331-8774-6c9b482161cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/SQuAD_Cleaned_DF.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1ce747-b873-4fb9-9fde-47bc3f368259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(data_path, 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36bf02-b7ef-4937-8693-832ec83fa347",
   "metadata": {},
   "source": [
    "## Ensure each word is lower case to allow lemmatization, stemming, and stopwords to occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd25d5c-b2c3-468f-8af2-6a964f8ce971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].str.lower()\n",
    "df['context'] = df['context'].str.lower()\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d350a-3e25-4b24-83f8-3d86fa4a6626",
   "metadata": {},
   "source": [
    "## Lemmatize, Stem, and Remove Stopwords from the question, context, and text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46919d8-1ff0-4787-b46c-3b57ac368d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stem_remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    This function performs lemmatization and stopword removal on text.\n",
    "     \n",
    "    Args:\n",
    "        text: A string containing the text to process.\n",
    "    \n",
    "    Returns:\n",
    "        A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)  # tokenize\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "                        if token not in stop_words]  # Lemmatize and remove stopwords\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_tokens = [ps.stem(token) for token in lemmatized_tokens] # Stem\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bcbc96-54d2-45eb-ac3f-4b6ab01353c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].apply(lemmatize_stem_remove_stopwords)\n",
    "df['context'] = df['context'].apply(lemmatize_stem_remove_stopwords)\n",
    "df['text'] = df['text'].apply(lemmatize_stem_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b3d05-ea25-42f3-bf89-4b7cb5d70cdb",
   "metadata": {},
   "source": [
    "## Check to see if the first context cell was lemmatized, stemmed, and has the stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9874a85b-b48c-4926-b474-38d18eb79ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['architectur',\n",
       " ',',\n",
       " 'school',\n",
       " 'cathol',\n",
       " 'charact',\n",
       " '.',\n",
       " 'atop',\n",
       " 'main',\n",
       " 'build',\n",
       " \"'s\",\n",
       " 'gold',\n",
       " 'dome',\n",
       " 'golden',\n",
       " 'statu',\n",
       " 'virgin',\n",
       " 'mari',\n",
       " '.',\n",
       " 'immedi',\n",
       " 'front',\n",
       " 'main',\n",
       " 'build',\n",
       " 'face',\n",
       " ',',\n",
       " 'copper',\n",
       " 'statu',\n",
       " 'christ',\n",
       " 'arm',\n",
       " 'uprais',\n",
       " 'legend',\n",
       " '``',\n",
       " 'venit',\n",
       " 'ad',\n",
       " 'omn',\n",
       " \"''\",\n",
       " '.',\n",
       " 'next',\n",
       " 'main',\n",
       " 'build',\n",
       " 'basilica',\n",
       " 'sacr',\n",
       " 'heart',\n",
       " '.',\n",
       " 'immedi',\n",
       " 'behind',\n",
       " 'basilica',\n",
       " 'grotto',\n",
       " ',',\n",
       " 'marian',\n",
       " 'place',\n",
       " 'prayer',\n",
       " 'reflect',\n",
       " '.',\n",
       " 'replica',\n",
       " 'grotto',\n",
       " 'lourd',\n",
       " ',',\n",
       " 'franc',\n",
       " 'virgin',\n",
       " 'mari',\n",
       " 'reputedli',\n",
       " 'appear',\n",
       " 'saint',\n",
       " 'bernadett',\n",
       " 'soubir',\n",
       " '1858.',\n",
       " 'end',\n",
       " 'main',\n",
       " 'drive',\n",
       " '(',\n",
       " 'direct',\n",
       " 'line',\n",
       " 'connect',\n",
       " '3',\n",
       " 'statu',\n",
       " 'gold',\n",
       " 'dome',\n",
       " ')',\n",
       " ',',\n",
       " 'simpl',\n",
       " ',',\n",
       " 'modern',\n",
       " 'stone',\n",
       " 'statu',\n",
       " 'mari',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['context'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a4c8d-a30e-44bb-b274-3d1936e89c21",
   "metadata": {},
   "source": [
    "## Output the lemmatized, stemmed, and removed stop words dataset to an updated .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7376b94-9b81-44da-a4ac-cc61ade765d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\638658\\AppData\\Local\\Temp\\ipykernel_37496\\582776807.py:1: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['index', 'question', 'context', 'text'], dtype='object')]\n",
      "\n",
      "  df.to_hdf('../data/SQuAD_Processed_DF.h5', key = 'df')\n"
     ]
    }
   ],
   "source": [
    "df.to_hdf('../data/SQuAD_Processed_DF.h5', key = 'df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
